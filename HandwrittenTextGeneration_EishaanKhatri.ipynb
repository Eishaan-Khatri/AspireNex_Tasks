{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMEQVS3SpsJh"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDR1-gmGqGSs"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MNxMT2SrrEbZ"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp /content/drive/MyDrive/ColabNotebooks/kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mff8j6I8rojm"
      },
      "outputs": [],
      "source": [
        "! kaggle datasets download landlord/handwriting-recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aNxLLvo68zh"
      },
      "outputs": [],
      "source": [
        "! unzip handwriting-recognition.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "h-lf5r2LB4kw"
      },
      "outputs": [],
      "source": [
        "# importing libraries\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from itertools import chain\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Embedding,  Dense, Flatten, Dropout, LSTM, TimeDistributed, BatchNormalization, Reshape\n",
        "from keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "g_2HtoCeARQy"
      },
      "outputs": [],
      "source": [
        "# Loading the dataset\n",
        "train = pd.read_csv('/content/written_name_train_v2.csv')\n",
        "test = pd.read_csv('/content/written_name_test_v2.csv')\n",
        "train_img_dir = '/content/train_v2/train'\n",
        "test_img_dir = '/content/test_v2/test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "p0bgmeCRusKx"
      },
      "outputs": [],
      "source": [
        "# Filtering out 'UNREADABLE' and NaN labels\n",
        "train = train[train['IDENTITY'] != 'UNREADABLE'].dropna()\n",
        "test = test[test['IDENTITY'] != 'UNREADABLE'].dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "o-MDgFqr2Mcv",
        "outputId": "d650cb92-941f-48d5-d9f4-e1052cc638bf"
      },
      "outputs": [],
      "source": [
        "# Function to convert name to vectors\n",
        "def name_to_vectors(name, max_len, char_to_index):\n",
        "    name = str(name)\n",
        "    vector = [char_to_index[char] for char in name if char in char_to_index]\n",
        "    # Padding\n",
        "    vector += [char_to_index[' ']] * (max_len - len(vector))\n",
        "    return vector\n",
        "\n",
        "# Function to convert image to binary\n",
        "def to_binary(image):\n",
        "    image = cv2.resize(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), dsize=(284, 62))\n",
        "    image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    _, image_binary = cv2.threshold(image_gray, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
        "    image_binary = image_binary / 255.0\n",
        "    return image_binary\n",
        "\n",
        "# Loading data and preprocess\n",
        "def load_data(data, img_dir, max_samples=None):\n",
        "    data_img = []\n",
        "    data_idt = []\n",
        "    num_samples = min(len(data), max_samples) if max_samples else len(data)\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        image_filename = data['FILENAME'].iloc[i]\n",
        "        image_path = os.path.join(img_dir, image_filename)\n",
        "        image = cv2.imread(image_path)\n",
        "        image = to_binary(image)\n",
        "        data_img.append(image)\n",
        "        data_idt.append(data['IDENTITY'].iloc[i])\n",
        "\n",
        "    return data_img, data_idt\n",
        "\n",
        "# Loading training and test data\n",
        "train_data_img, train_data_idt = load_data(train, train_img_dir)\n",
        "test_data_img, test_data_idt = load_data(test, test_img_dir)\n",
        "\n",
        "# Calculating the maximum length of names in train_data_idt\n",
        "max_len = max(len(str(name)) for name in train_data_idt)\n",
        "\n",
        "# Get unique characters and create mapping\n",
        "all_data = train_data_idt + test_data_idt\n",
        "all_characters = ''.join(all_data)\n",
        "unique_characters = sorted(set(all_characters))\n",
        "char_to_index = {char: i for i, char in enumerate(unique_characters)}\n",
        "index_to_char = {i: char for i, char in enumerate(unique_characters)}\n",
        "\n",
        "# Creating vectors for each name in train_data_idt and test_data_idt\n",
        "vectors_list_train_idt = [name_to_vectors(name, max_len, char_to_index) for name in train_data_idt]\n",
        "vectors_list_test_idt = [name_to_vectors(name, max_len, char_to_index) for name in test_data_idt]\n",
        "\n",
        "# Converting to numpy arrays\n",
        "vectors_list_train_idt = np.array(vectors_list_train_idt)\n",
        "vectors_list_test_idt = np.array(vectors_list_test_idt)\n",
        "\n",
        "# To One-hot encode the output\n",
        "vectors_list_train_idt_one_hot = to_categorical(vectors_list_train_idt, num_classes=len(unique_characters))\n",
        "vectors_list_test_idt_one_hot = to_categorical(vectors_list_test_idt, num_classes=len(unique_characters))\n",
        "\n",
        "# Model\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(unique_characters), 64, input_length=max_len))\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(len(unique_characters), activation='softmax')))\n",
        "\n",
        "# Compilation of the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(vectors_list_train_idt, vectors_list_train_idt_one_hot, epochs=1, batch_size=32, validation_data=(vectors_list_test_idt, vectors_list_test_idt_one_hot))\n",
        "# Function to generate text\n",
        "def generate_text(model, start_string, char_to_index, index_to_char, max_len, num_generate=100):\n",
        "    input_eval = [char_to_index[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    text_generated = []\n",
        "\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        predicted_id = tf.random.categorical(predictions[:, -1, :], num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        input_eval = tf.concat([input_eval, tf.expand_dims([predicted_id], 0)], axis=1)\n",
        "        text_generated.append(index_to_char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOFzG2FPAHUt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
